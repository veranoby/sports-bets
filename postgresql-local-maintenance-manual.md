# üìã MANUAL DE MANTENIMIENTO: PostgreSQL Local (Semanal)

**Para:** GalloBets con Nginx RTMP + PostgreSQL local
**Frecuencia:** Actividades semanales (30 min/semana)
**Responsable:** Veranoby o DevOps designado

---

## üìå MIGRACI√ìN COMPLETADA (2025-11-25)

**Status:** ‚úÖ Database migration from Neon Tech to PostgreSQL local COMPLETED

**Migration details:**
- Schema: migracion.sql with 22 ENUMs, 19 tables, 26 FKs created successfully
- Data imported: 5 tables from Neon Tech to local (users:9, system_settings:88, subscriptions:2, articles:1, membership_change_requests:2)
- Method: Manual CSV export from Neon SQL Editor ‚Üí COPY FROM in local PostgreSQL
- Database location: 127.0.0.1:5432 (user: postgres, password: 0102Mina)
- Backup of old config preserved: /tmp/nginx.conf.backup

**Files updated:**
- backend/.env: DATABASE_URL, pricing (5.00/10.00), RTMP/HLS URLs
- backend/src/config/envValidator.ts: Updated defaults for subscription pricing
- Nginx config: RTMP module added, HLS generation enabled

---

## üîç TAREAS SEMANALES (Lunes 10:00 AM recomendado)

### ACTIVIDAD 1: Verificar Salud del Sistema (5 min)

**Comando:**
```bash
# SSH a tu servidor dedicado
ssh user@your-nginx-server.com

# Verificar si PostgreSQL est√° corriendo
sudo systemctl status postgresql

# Ver si hay conexiones activas
sudo -u postgres psql -c "SELECT datname, count(*) FROM pg_stat_activity GROUP BY datname;"

# Ver tama√±o de la BD
sudo -u postgres psql -c "SELECT pg_size_pretty(pg_database_size('gallobets'));"
```

**Qu√© buscar:**
- Status: debe decir `active (running)`
- Conexiones: <20 (si ves >50, hay problema)
- Tama√±o: debe crecer lentamente (no explotar de repente)

**Si ve problema:** ‚ö†Ô∏è Ir a TROUBLESHOOTING al final

---

### ACTIVIDAD 2: Backup Autom√°tico (3 min setup, 0 min semanal)

**Setup (HACER UNA SOLA VEZ):**
```bash
# Crear directorio para backups
sudo mkdir -p /var/backups/postgres
sudo chown postgres:postgres /var/backups/postgres

# Crear script de backup
sudo tee /usr/local/bin/backup-gallobets.sh > /dev/null << 'EOF'
#!/bin/bash
BACKUP_DIR="/var/backups/postgres"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/gallobets_$DATE.sql"

# Full backup comprimido
sudo -u postgres pg_dump gallobets | gzip > "$BACKUP_FILE.gz"

# Mantener √∫ltimos 4 backups (28 d√≠as si es 1/semana)
ls -t "$BACKUP_DIR"/gallobets_*.sql.gz | tail -n +5 | xargs -r rm

echo "‚úÖ Backup completado: $BACKUP_FILE.gz"
EOF

sudo chmod +x /usr/local/bin/backup-gallobets.sh

# Agregar a cron (cada domingo 11:00 PM)
sudo crontab -e
# Agregar esta l√≠nea:
# 0 23 * * 0 /usr/local/bin/backup-gallobets.sh

# Verificar que est√° instalado
sudo crontab -l | grep backup
```

**Semanal (SOLO MONITOREAR):**
```bash
# Verificar que backup se ejecut√≥
ls -lh /var/backups/postgres/

# Debe haber un archivo nuevo cada semana con tama√±o >100KB
# Ejemplo: gallobets_20251127_230001.sql.gz (12.5 MB)
```

**Verificar integridad cada mes:**
```bash
# Tomar el backup m√°s reciente y probar restore (NO en producci√≥n):
# cp /var/backups/postgres/gallobets_LATEST.sql.gz /tmp/
# gzip -d /tmp/gallobets_LATEST.sql.gz
# sudo -u postgres psql testdb < /tmp/gallobets_LATEST.sql
# Si no hay errores, backup est√° OK
```

---

### ACTIVIDAD 3: Monitorear Espacio en Disco (2 min)

**Comando semanal:**
```bash
# Ver espacio total del servidor
df -h /

# Ver tama√±o de PostgreSQL
du -sh /var/lib/postgresql/

# Ver tama√±o de datos espec√≠fico
sudo -u postgres psql -c "
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;"
```

**Valores normales:**
- Disco total: Si es 500GB SSD ‚Üí m√°ximo 80% lleno es advertencia
- PostgreSQL: Debe crecer ~2-5% por semana con usuarios activos
- Tablas grandes: `events`, `bets`, `users` son las principales

**Si ves advertencia (>80% lleno):**
- ‚ö†Ô∏è Ir a TROUBLESHOOTING: "Disco casi lleno"

---

### ACTIVIDAD 4: Verificar Logs de Errores (3 min)

**Comando semanal:**
```bash
# Ver √∫ltimos 100 errores en PostgreSQL
sudo tail -100 /var/log/postgresql/postgresql.log | grep ERROR

# Ver si hay errores de conexi√≥n
sudo tail -50 /var/log/postgresql/postgresql.log | grep "connection"

# Ver queries lentas (si est√°n loggadas)
sudo tail -50 /var/log/postgresql/postgresql.log | grep "duration:"
```

**Qu√© buscar:**
- ‚ùå `FATAL`: Problema serio ‚Üí Investigar inmediatamente
- ‚ùå `ERROR: out of memory`: Base de datos saturada
- ‚ö†Ô∏è `WARNING`: Minor issue, monitor pr√≥xima semana
- ‚úÖ `LOG`: Mensajes informativos normales

---

### ACTIVIDAD 5: Ejecutar VACUUM y ANALYZE (5 min)

**Comando semanal (ejecutar despu√©s de backup):**
```bash
# VACUUM: Limpia espacio muerto
# ANALYZE: Actualiza estad√≠sticas para query optimizer
sudo -u postgres psql gallobets << 'EOF'
\timing on
VACUUM ANALYZE;
EOF
```

**Esperado:**
- Duraci√≥n: 2-5 minutos (depende tama√±o BD)
- Output: `VACUUM` sin errores
- Si tarda >10 min ‚Üí BD tiene demasiados datos muertos ‚Üí quiz√°s necesitas `REINDEX`

**Alternativa automatizada (cron, en lugar de manual):**
```bash
# Agregar a crontab (mi√©rcoles 2:00 AM):
# 0 2 * * 3 sudo -u postgres psql gallobets -c "VACUUM ANALYZE;"
```

---

### ACTIVIDAD 6: Monitorear Conexiones Activas (2 min)

**Comando semanal:**
```bash
# Ver conexiones actuales
sudo -u postgres psql -c "
SELECT
    pid,
    usename,
    application_name,
    state,
    query_start,
    query
FROM pg_stat_activity
WHERE state IS NOT NULL
ORDER BY query_start;"

# Contar por aplicaci√≥n
sudo -u postgres psql -c "
SELECT application_name, COUNT(*) as connections
FROM pg_stat_activity
WHERE state IS NOT NULL
GROUP BY application_name;"
```

**Valores normales:**
- Node.js backend: 5-10 conexiones
- Admin panel: 1-3 conexiones
- Total: <20 conexiones normales

**Si ves >50 conexiones:**
- ‚ö†Ô∏è Hay conexiones "colgadas" ‚Üí Ir a TROUBLESHOOTING

---

### ACTIVIDAD 7: Validar √çndices (2 min)

**Comando semanal:**
```bash
# Ver √≠ndices que NO est√°n siendo usados
sudo -u postgres psql gallobets << 'EOF'
SELECT
    t.relname as table,
    i.relname as index,
    idx.idx_scan
FROM pg_class t
JOIN pg_index x ON t.oid = x.indrelid
JOIN pg_class i ON i.oid = x.indexrelid
LEFT JOIN pg_stat_user_indexes idx ON idx.indexrelname = i.relname
WHERE idx.idx_scan = 0
AND t.relname NOT LIKE 'pg_%'
ORDER BY t.relname;
EOF
```

**Qu√© buscar:**
- Si ves √≠ndices con `idx_scan = 0` ‚Üí son innecesarios (pueden deletarse despu√©s, no urgente)
- Normal ver 1-5 √≠ndices no usados

---

## ‚ö†Ô∏è TROUBLESHOOTING R√ÅPIDO

### Problema: "Conexiones colgadas"
```bash
# Matar conexi√≥n espec√≠fica
sudo -u postgres psql -c "
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE pid <> pg_backend_pid()
  AND state = 'idle'
  AND query_start < now() - interval '1 hour';"

# Matar TODAS las conexiones excepto la tuya
sudo -u postgres psql -c "
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE pid <> pg_backend_pid();"
```

### Problema: "Disco casi lleno"
```bash
# Ver qu√© est√° ocupando espacio
sudo du -sh /var/lib/postgresql/*

# Opci√≥n 1: Limpiar logs viejos
sudo rm /var/log/postgresql/postgresql-*.log

# Opci√≥n 2: Comprimir logs
sudo find /var/log/postgresql -name "*.log" -exec gzip {} \;

# Opci√≥n 3: REINDEX (LENTO, 30-60 min)
sudo -u postgres psql gallobets -c "REINDEX DATABASE gallobets;"
```

### Problema: "PostgreSQL no inicia"
```bash
# Ver error espec√≠fico
sudo systemctl status postgresql

# Ver logs detallados
sudo journalctl -u postgresql -n 50

# Intentar restart
sudo systemctl restart postgresql

# Si falla, comprobar integridad
sudo -u postgres pg_ctl -D /var/lib/postgresql/17/main status
```

### Problema: "Queries muy lentas"
```bash
# Ver queries m√°s lentas actualmente
sudo -u postgres psql gallobets << 'EOF'
SELECT
    pid,
    now() - pg_stat_activity.query_start AS duration,
    query,
    state
FROM pg_stat_activity
WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';
EOF

# Matar query que tarda mucho
# SELECT pg_terminate_backend(pid_number);
```

---

## üìä CHECKLIST SEMANAL (Copia y pega)

```markdown
SEMANA DE: [FECHA]

[ ] Lunes 10:00 AM - Verificar salud del sistema
    - systemctl status postgresql
    - Ver conexiones activas
    - Ver tama√±o BD

[ ] Lunes - Monitorear backups
    - ls -lh /var/backups/postgres/
    - Verificar backup reciente existe

[ ] Cualquier d√≠a - Monitorear espacio
    - df -h /
    - du -sh /var/lib/postgresql/

[ ] Cualquier d√≠a - Revisar logs
    - tail -100 postgresql.log | grep ERROR

[ ] Mi√©rcoles 2:00 AM - VACUUM ANALYZE
    - Ejecutado v√≠a cron

[ ] Cualquier d√≠a - Conexiones
    - pg_stat_activity: <20 conexiones

[ ] Cualquier d√≠a - √çndices
    - Ver √≠ndices no usados (informativo)

PROBLEMAS ENCONTRADOS: [NONE / LIST]
ACCIONES TOMADAS: [LIST]
```

---

## üö® M√âTRICAS CR√çTICAS A MONITOREAR (Escalabilidad 2025-11-20)

### Tabla de Monitoreo por Fase de Crecimiento

| M√©trica | MVP <500u | Scaling 500-800u | Production 1000u | Trigger Upgrade |
|---------|-----------|------------------|------------------|-----------------|
| **CPU total (Nginx + PG)** | <40% ‚úÖ | 40-70% ‚ö†Ô∏è | 70%+ üî¥ | At 70% sustained ‚Üí Plan server upgrade |
| **Conexiones activas DB** | <20 ‚úÖ | 20-50 ‚ö†Ô∏è | 50-200+ üî¥ | At max:20 ‚Üí Plan PgBouncer OR upgrade |
| **Espacio disco libre** | >50% ‚úÖ | 20-50% ‚ö†Ô∏è | <20% üî¥ | At 80% full ‚Üí CRITICAL - Upgrade |
| **DB growth/semana** | +2-5% | +5-10% ‚ö†Ô∏è | +10%+ üî¥ | Monitor for exponential growth |
| **VACUUM duration** | <5 min ‚úÖ | 5-10 min ‚ö†Ô∏è | >10 min üî¥ | If >10 min ‚Üí REINDEX needed |
| **Disk I/O (IOPS)** | ~500 | 500-1000 ‚ö†Ô∏è | 1000+ üî¥ | Monitor SSD saturation |
| **PostgreSQL RAM** | 8-10GB | 10-16GB | 12-16GB | Scales with workload |
| **Network bandwidth** | 750 Mbps | 750-1500 Mbps | 1500+ Mbps | Phase 1B: Add Bunny CDN @ >750 Mbps |
| **Error logs/week** | 0-2 | 3-5 ‚ö†Ô∏è | 5+ üî¥ | Investigate error patterns |
| **Backup integrity** | Monthly test | Monthly test | Weekly test | Always verify restore |

---

## üìà ESCALABILIDAD: TRIGGERES PARA UPGRADE (2025-11-20)

### PHASE 1B: Agregar Bunny CDN (Months 5-6, if reaching 750 Mbps)

**Trigger signals (PHASE 1B):**
- ‚úÖ Nginx server CPU >60% constantly
- ‚úÖ Network bandwidth output >500 Mbps average
- ‚úÖ Latency for users >200ms in other regions
- ‚úÖ GalloBets approaching 600+ concurrent users

**When Phase 1B triggered:**
1. Agregar Bunny CDN Pull Zone ($108-216/mo)
2. Apuntar Bunny a tu servidor Nginx
3. Video.js descarga desde Bunny en lugar de Nginx
4. Tu Nginx solo ve tr√°fico de Bunny (no direct user connections)

**Cost at Phase 1B:** $155-195 (server) + $108-216 (CDN) = **$263-411/mo**

**Impact:** Servidor dedicado $155-195 sigue siendo suficiente (solo Nginx genera HLS)

---

### PHASE 2A: Server Upgrade (Months 6+, if 3 triggers hit)

**CRITICAL: Plan upgrade BEFORE hitting these simultaneously:**

| Trigger | Threshold | Action | Timeline |
|---------|-----------|--------|----------|
| **CPU saturation** | Nginx + PostgreSQL > 70% sustained for 1 week | Start upgrade planning, 2-week lead time | üî¥ PLAN immediately |
| **Disk space** | Free space < 20% (< 100GB on 500GB disk) | üî¥ CRITICAL - Upgrade within 48h OR move HLS to S3 | üî¥ URGENT |
| **Connection pool** | Database connections at max:20 consistently | Add PgBouncer ($50-100/mo) OR plan full upgrade | ‚ö†Ô∏è WARN |

**Server Upgrade Path:**
- **Current:** 8-core, 32GB RAM, 500GB SSD = $155-195/mo
- **Upgrade to:** 16-core, 64GB RAM, 2TB SSD = $400-500/mo
- **Migration time:** 2-4 hours (backup ‚Üí restore ‚Üí DNS switch)
- **Downtime:** < 30 minutes
- **Total cost Phase 2:** $400-500 (server) + $108-216 (CDN) = **$508-716/mo**

**At 16-core, 64GB, 2TB you support 1000+ concurrent users:**
- 16 cores = 2√ó headroom (CPU only reaches 50% under load)
- 64GB RAM = PostgreSQL unlimited buffers
- 2TB SSD = 12+ months of data growth

---

### PHASE 2B: PgBouncer (Emergency intermediate, if pool exhausted but CPU <70%)

**When to use PgBouncer ONLY:**
- Connection pool maxed at 20
- CPU is still <70% (not saturated)
- Need quick fix before full server upgrade

**What PgBouncer does:**
- ‚úÖ Solves connection pool bottleneck
- ‚ùå Does NOT solve CPU saturation
- ‚ùå Does NOT solve disk space filling
- Extends current server to ~800 users max (still needs Phase 2A upgrade @ 1000 users)

**Cost:** $50-100/mo (separate VPS for pooling proxy)

---

### PHASE 2C: Revisit Neon.tech (Alternative path if maintenance burden)

**Alternative to server upgrade:**
- Keep Nginx RTMP local ($155-195/mo)
- Move PostgreSQL to Neon.tech ($50-100/mo) managed
- Keep Bunny CDN ($108-216/mo)
- **Total:** $313-511/mo

**Trade-off:**
- Costs $50-200/mo MORE than local PostgreSQL
- BUT: Eliminates 30 min/week maintenance
- AND: Scales to 1000+ without server upgrade
- AND: 99.99% SLA + automatic failover

**Decision criteria:** If maintenance becomes burden OR if rapid growth beyond 800 users predicted

---

## üí° AUTOMATIZACI√ìN RECOMENDADA

**Ya configurado (cron):**
- ‚úÖ Backup autom√°tico: domingo 23:00
- ‚úÖ VACUUM ANALYZE: mi√©rcoles 02:00

**Opcional (si quieres monitoreo avanzado):**
```bash
# Instalar Prometheus para PostgreSQL
sudo apt install prometheus-postgres-exporter

# Configurar Grafana para dashboards
# (Pero para MVP, cron + checklist semanal es suficiente)
```

---

## üìû RESUMEN FINAL: MANTENIMIENTO + ESCALABILIDAD (2025-11-20)

### MANTENIMIENTO SEMANAL: 30 minutos

1. **Verificar salud** (5 min)
   - systemctl status postgresql
   - Ver conexiones activas
   - Ver tama√±o BD

2. **Monitorear backups** (3 min)
   - ls -lh /var/backups/postgres/
   - Verificar backup reciente existe

3. **Espacio en disco** (2 min)
   - df -h /
   - du -sh /var/lib/postgresql/

4. **Revisar logs** (3 min)
   - tail -100 postgresql.log | grep ERROR

5. **VACUUM** (automatizado v√≠a cron)
   - Mi√©rcoles 02:00 AM

6. **Conexiones activas** (2 min)
   - pg_stat_activity: comparar contra tabla de l√≠mites

7. **√çndices** (2 min)
   - Ver √≠ndices no usados (informativo)

### MONITOREO PARA ESCALABILIDAD

**Comparar semanalmente contra tabla de M√âTRICAS CR√çTICAS (l√≠nea 340+):**

- Si **CPU > 70%:** üî¥ Start server upgrade planning (2-week lead time)
- Si **Conexiones > 20 m√°x:** ‚ö†Ô∏è Plan PgBouncer OR upgrade
- Si **Disco < 20% libre:** üî¥ CRITICAL - Upgrade within 48h

**Phase 1 (MVP):**
- Monitor CPU: target <40%
- Monitor pool: target <20 connections
- Monitor disk: target >50% free

**Phase 1B (Add CDN @ 750 Mbps):**
- Bunny CDN triggered: CPU >60%
- Cost: +$108-216/mo

**Phase 2A (Server upgrade @ 800 users):**
- CPU > 70% sustained OR disk <20% free OR pool exhausted
- Upgrade to: 16-core, 64GB, 2TB = $400-500/mo
- Downtime: < 30 min

**Phase 2C (Alternative: use Neon.tech):**
- If maintenance becomes burden
- Cost: +$50-100/mo (more than local, but managed)

---

## ‚úÖ DOCUMENTACI√ìN ACTUALIZADA (2025-11-20)

**Archivos actualizados con an√°lisis de escalabilidad:**
- ‚úÖ `brain/sdd_system.json` - Escalability section + upgrade triggers
- ‚úÖ `brain/prd_system.json` - Cost/phase breakdown for all 3 scenarios
- ‚úÖ `postgresql-local-maintenance-manual.md` - Metrics + triggers + alternatives

**Plan actual:**
- ‚úÖ PostgreSQL local PERMANENT (no Neon.tech unless maintenance becomes burden)
- ‚úÖ Weekly monitoring checklist embedded
- ‚úÖ Clear upgrade triggers (CPU, disk, connections)
- ‚úÖ Cost projections for all phases (Phase 1 ‚Üí Phase 3)

---
